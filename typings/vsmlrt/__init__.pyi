"""
This type stub file was generated by pyright.
"""

import copy
import enum
import math
import os
import os.path
import platform
import subprocess
import sys
import tempfile
import time
import typing
import zlib
from dataclasses import dataclass, field
from fractions import Fraction

import vapoursynth as vs
from vapoursynth import core

__version__ = ...
__all__ = [
    "Backend",
    "BackendV2",
    "Waifu2x",
    "Waifu2xModel",
    "DPIR",
    "DPIRModel",
    "RealESRGAN",
    "RealESRGANModel",
    "RealESRGANv2",
    "RealESRGANv2Model",
    "CUGAN",
    "RIFE",
    "RIFEModel",
    "RIFEMerge",
    "SAFA",
    "SAFAModel",
    "SAFAAdaptiveMode",
    "SCUNet",
    "SCUNetModel",
    "SwinIR",
    "SwinIRModel",
    "ArtCNN",
    "ArtCNNModel",
    "inference",
    "flexible_inference",
]

def get_plugins_path() -> str: ...

plugins_path: str = ...
trtexec_path: str = ...
migraphx_driver_path: str = ...
tensorrt_rtx_path: str = ...
models_path: str = ...

class Backend:
    @dataclass(frozen=False)
    class ORT_CPU:
        """backend for cpus"""

        num_streams: int = ...
        verbosity: int = ...
        fp16: bool = ...
        fp16_blacklist_ops: typing.Optional[typing.Sequence[str]] = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class ORT_CUDA:
        """backend for nvidia gpus

        basic performance tuning:
        set fp16 = True (on RTX GPUs)

        Semantics of `fp16`:
            Enabling `fp16` will use a built-in quantization that converts a fp32 onnx to a fp16 onnx.
            If the input video is of half-precision floating-point format,
            the generated fp16 onnx will use fp16 input.
            The output format can be controlled by the `output_format` option (0 = fp32, 1 = fp16).

            Disabling `fp16` will not use the built-in quantization.
            However, if the onnx file itself uses fp16 for computation,
            the actual computation will be done in fp16.
            In this case, the input video format should match the input format of the onnx,
            and the output format is inferred from the onnx.
        """

        device_id: int = ...
        cudnn_benchmark: bool = ...
        num_streams: int = ...
        verbosity: int = ...
        fp16: bool = ...
        use_cuda_graph: bool = ...
        fp16_blacklist_ops: typing.Optional[typing.Sequence[str]] = ...
        prefer_nhwc: bool = ...
        output_format: int = ...
        tf32: bool = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class OV_CPU:
        """backend for x86 cpus

        basic performance tuning:
        set bf16 = True (on Zen4)
        increase num_streams
        """

        fp16: bool = ...
        num_streams: typing.Union[int, str] = ...
        bind_thread: bool = ...
        fp16_blacklist_ops: typing.Optional[typing.Sequence[str]] = ...
        bf16: bool = ...
        num_threads: int = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class TRT:
        """backend for nvidia gpus

        basic performance tuning:
        set fp16 = True (on RTX GPUs)
        increase num_streams
        increase workspace
        set use_cuda_graph = True
        """

        max_shapes: typing.Optional[typing.Tuple[int, int]] = ...
        opt_shapes: typing.Optional[typing.Tuple[int, int]] = ...
        fp16: bool = ...
        device_id: int = ...
        workspace: typing.Optional[int] = ...
        verbose: bool = ...
        use_cuda_graph: bool = ...
        num_streams: int = ...
        use_cublas: bool = ...
        static_shape: bool = ...
        tf32: bool = ...
        log: bool = ...
        use_cudnn: bool = ...
        use_edge_mask_convolutions: bool = ...
        use_jit_convolutions: bool = ...
        heuristic: bool = ...
        output_format: int = ...
        min_shapes: typing.Tuple[int, int] = ...
        faster_dynamic_shapes: bool = ...
        force_fp16: bool = ...
        builder_optimization_level: int = ...
        max_aux_streams: typing.Optional[int] = ...
        short_path: typing.Optional[bool] = ...
        bf16: bool = ...
        custom_env: typing.Dict[str, str] = ...
        custom_args: typing.List[str] = ...
        engine_folder: typing.Optional[str] = ...
        max_tactics: typing.Optional[int] = ...
        tiling_optimization_level: int = ...
        l2_limit_for_tiling: int = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class OV_GPU:
        """backend for nvidia gpus

        basic performance tuning:
        set fp16 = True
        increase num_streams
        """

        fp16: bool = ...
        num_streams: typing.Union[int, str] = ...
        device_id: int = ...
        fp16_blacklist_ops: typing.Optional[typing.Sequence[str]] = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class NCNN_VK:
        """backend for vulkan devices

        basic performance tuning:
        set fp16 = True (on modern GPUs)
        increase num_streams
        """

        fp16: bool = ...
        device_id: int = ...
        num_streams: int = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class ORT_DML:
        """backend for directml (d3d12) devices"""

        device_id: int = ...
        num_streams: int = ...
        verbosity: int = ...
        fp16: bool = ...
        fp16_blacklist_ops: typing.Optional[typing.Sequence[str]] = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class MIGX:
        """backend for amd gpus

        basic performance tuning:
        set fp16 = True
        """

        device_id: int = ...
        fp16: bool = ...
        opt_shapes: typing.Optional[typing.Tuple[int, int]] = ...
        fast_math: bool = ...
        exhaustive_tune: bool = ...
        num_streams: int = ...
        short_path: typing.Optional[bool] = ...
        custom_env: typing.Dict[str, str] = ...
        custom_args: typing.List[str] = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class OV_NPU:
        """backend for intel npus"""

        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class ORT_COREML:
        """backend for coreml"""

        num_streams: int = ...
        verbosity: int = ...
        fp16: bool = ...
        fp16_blacklist_ops: typing.Optional[typing.Sequence[str]] = ...
        ml_program: int = ...
        supports_onnx_serialization: bool = ...

    @dataclass(frozen=False)
    class TRT_RTX:
        """backend for nvidia rtx gpus

        basic performance tuning:
        set fp16 = True
        increase num_streams
        increase workspace
        set use_cuda_graph = True
        """

        fp16: bool = ...
        device_id: int = ...
        workspace: typing.Optional[int] = ...
        verbose: bool = ...
        use_cuda_graph: bool = ...
        num_streams: int = ...
        use_edge_mask_convolutions: bool = ...
        builder_optimization_level: int = ...
        max_aux_streams: typing.Optional[int] = ...
        short_path: typing.Optional[bool] = ...
        custom_env: typing.Dict[str, str] = ...
        custom_args: typing.List[str] = ...
        engine_folder: typing.Optional[str] = ...
        max_tactics: typing.Optional[int] = ...
        tiling_optimization_level: int = ...
        l2_limit_for_tiling: int = ...
        supports_onnx_serialization: bool = ...

backendT = typing.Union[
    Backend.OV_CPU,
    Backend.ORT_CPU,
    Backend.ORT_CUDA,
    Backend.TRT,
    Backend.OV_GPU,
    Backend.NCNN_VK,
    Backend.ORT_DML,
    Backend.MIGX,
    Backend.OV_NPU,
    Backend.ORT_COREML,
    Backend.TRT_RTX,
]
fallback_backend: typing.Optional[backendT] = ...

@enum.unique
class Waifu2xModel(enum.IntEnum):
    anime_style_art = ...
    anime_style_art_rgb = ...
    photo = ...
    upconv_7_anime_style_art_rgb = ...
    upconv_7_photo = ...
    upresnet10 = ...
    cunet = ...
    swin_unet_art = ...
    swin_unet_photo = ...
    swin_unet_photo_v2 = ...
    swin_unet_art_scan = ...

def Waifu2x(
    clip: vs.VideoNode,
    noise: typing.Literal[-1, 0, 1, 2, 3] = ...,
    scale: typing.Literal[1, 2, 4] = ...,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: Waifu2xModel = ...,
    backend: backendT = ...,
    preprocess: bool = ...,
) -> vs.VideoNode: ...
@enum.unique
class DPIRModel(enum.IntEnum):
    drunet_gray = ...
    drunet_color = ...
    drunet_deblocking_grayscale = ...
    drunet_deblocking_color = ...

def DPIR(
    clip: vs.VideoNode,
    strength: typing.Optional[typing.Union[typing.SupportsFloat, vs.VideoNode]],
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: DPIRModel = ...,
    backend: backendT = ...,
) -> vs.VideoNode: ...
@enum.unique
class RealESRGANModel(enum.IntEnum):
    animevideo_xsx2 = ...
    animevideo_xsx4 = ...
    animevideov3 = ...
    animejanaiV2L1 = ...
    animejanaiV2L2 = ...
    animejanaiV2L3 = ...
    animejanaiV3_HD_L1 = ...
    animejanaiV3_HD_L2 = ...
    animejanaiV3_HD_L3 = ...
    Ani4Kv2_G6i2_Compact = ...
    Ani4Kv2_G6i2_UltraCompact = ...

RealESRGANv2Model = RealESRGANModel

def RealESRGAN(
    clip: vs.VideoNode,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: RealESRGANv2Model = ...,
    backend: backendT = ...,
    scale: typing.Optional[float] = ...,
) -> vs.VideoNode: ...

RealESRGANv2 = ...

def CUGAN(
    clip: vs.VideoNode,
    noise: typing.Literal[-1, 0, 1, 2, 3] = ...,
    scale: typing.Literal[2, 3, 4] = ...,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    backend: backendT = ...,
    alpha: float = ...,
    version: typing.Literal[1, 2] = ...,
    conformance: bool = ...,
) -> vs.VideoNode:
    """
    denoising strength: 0 < -1 < 1 < 2 < 3

    version: (1 or 2)
        1 -> legacy,
        2 -> pro (only models for "noise" in [-1, 0, 3] and "scale" in [2, 3] are published currently)
    """
    ...

def get_rife_input(clip: vs.VideoNode) -> typing.List[vs.VideoNode]: ...
@enum.unique
class RIFEModel(enum.IntEnum):
    """
    Starting from RIFE v4.12 lite, this interface does not provide forward compatiblity in enum values.
    """

    v4_0 = ...
    v4_2 = ...
    v4_3 = ...
    v4_4 = ...
    v4_5 = ...
    v4_6 = ...
    v4_7 = ...
    v4_8 = ...
    v4_9 = ...
    v4_10 = ...
    v4_11 = ...
    v4_12 = ...
    v4_12_lite = ...
    v4_13 = ...
    v4_13_lite = ...
    v4_14 = ...
    v4_14_lite = ...
    v4_15 = ...
    v4_15_lite = ...
    v4_16_lite = ...
    v4_17 = ...
    v4_17_lite = ...
    v4_18 = ...
    v4_19 = ...
    v4_20 = ...
    v4_21 = ...
    v4_22 = ...
    v4_22_lite = ...
    v4_23 = ...
    v4_24 = ...
    v4_25 = ...
    v4_25_lite = ...
    v4_25_heavy = ...
    v4_26 = ...
    v4_26_heavy = ...

def RIFEMerge(
    clipa: vs.VideoNode,
    clipb: vs.VideoNode,
    mask: vs.VideoNode,
    scale: float = ...,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: RIFEModel = ...,
    backend: backendT = ...,
    ensemble: bool = ...,
    _implementation: typing.Optional[typing.Literal[1, 2]] = ...,
) -> vs.VideoNode:
    """temporal MaskedMerge-like interface for the RIFE model

    Its semantics is similar to core.std.MaskedMerge(clipa, clipb, mask, first_plane=True),
    except that it merges the two clips in the time domain and you specify the "mask" based
    on the time point of the resulting clip (range (0,1)) between the two clips.
    """
    ...

def RIFE(
    clip: vs.VideoNode,
    multi: typing.Union[int, Fraction] = ...,
    scale: float = ...,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: RIFEModel = ...,
    backend: backendT = ...,
    ensemble: bool = ...,
    video_player: bool = ...,
    _implementation: typing.Optional[typing.Literal[1, 2]] = ...,
) -> vs.VideoNode:
    """RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation

    multi, scale is based on vs-rife.

    For the best results, you need to perform scene detection on the input clip
    (e.g. misc.SCDetect, mv.SCDetection) before passing it to RIFE.
    Also note that the quality of result is strongly dependent on high quality
    scene detection and you might need to tweak the scene detection parameters
    and/or filter to achieve the best quality.

    Args:
        multi: Multiple of the frame counts, can be a fractions.Fraction.
            Default: 2.

        scale: Controls the process resolution for optical flow model.
            32 / fractions.Fraction(scale) must be an integer.
            scale=0.5 is recommended for 4K video.

        _implementation: (None, 1 or 2, experimental and maybe removed in the future)
            Switch between different onnx implementation.
            Implmementation will be selected based on internal heuristic if it is None.
    """
    ...

@enum.unique
class SAFAModel(enum.IntEnum):
    v0_1 = ...
    v0_2 = ...
    v0_3 = ...
    v0_4 = ...
    v0_5 = ...

@enum.unique
class SAFAAdaptiveMode(enum.IntEnum):
    non_adaptive = ...
    adaptive1x = ...
    adaptive = ...

def SAFA(
    clip: vs.VideoNode,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: SAFAModel = ...,
    adaptive: SAFAAdaptiveMode = ...,
    backend: backendT = ...,
) -> vs.VideoNode:
    """SAFA: Scale-Adaptive Feature Aggregation for Efficient Space-Time Video Super-Resolution"""
    ...

@enum.unique
class SCUNetModel(enum.IntEnum):
    scunet_color_15 = ...
    scunet_color_25 = ...
    scunet_color_50 = ...
    scunet_color_real_psnr = ...
    scunet_color_real_gan = ...
    scunet_gray_15 = ...
    scunet_gray_25 = ...
    scunet_gray_50 = ...

def SCUNet(
    clip: vs.VideoNode,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: SCUNetModel = ...,
    backend: backendT = ...,
) -> vs.VideoNode:
    """Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis

    Unlike vs-scunet v1.0.0, the default model is set to scunet_color_real_psnr due to the color shift.
    """
    ...

@enum.unique
class SwinIRModel(enum.IntEnum):
    lightweightSR_DIV2K_s64w8_SwinIR_S_x2 = ...
    lightweightSR_DIV2K_s64w8_SwinIR_S_x3 = ...
    lightweightSR_DIV2K_s64w8_SwinIR_S_x4 = ...
    realSR_BSRGAN_DFOWMFC_s64w8_SwinIR_L_x4_GAN = ...
    realSR_BSRGAN_DFOWMFC_s64w8_SwinIR_L_x4_PSNR = ...
    classicalSR_DF2K_s64w8_SwinIR_M_x2 = ...
    classicalSR_DF2K_s64w8_SwinIR_M_x3 = ...
    classicalSR_DF2K_s64w8_SwinIR_M_x4 = ...
    classicalSR_DF2K_s64w8_SwinIR_M_x8 = ...
    realSR_BSRGAN_DFO_s64w8_SwinIR_M_x2_GAN = ...
    realSR_BSRGAN_DFO_s64w8_SwinIR_M_x2_PSNR = ...
    realSR_BSRGAN_DFO_s64w8_SwinIR_M_x4_GAN = ...
    realSR_BSRGAN_DFO_s64w8_SwinIR_M_x4_PSNR = ...
    grayDN_DFWB_s128w8_SwinIR_M_noise15 = ...
    grayDN_DFWB_s128w8_SwinIR_M_noise25 = ...
    grayDN_DFWB_s128w8_SwinIR_M_noise50 = ...
    colorDN_DFWB_s128w8_SwinIR_M_noise15 = ...
    colorDN_DFWB_s128w8_SwinIR_M_noise25 = ...
    colorDN_DFWB_s128w8_SwinIR_M_noise50 = ...
    CAR_DFWB_s126w7_SwinIR_M_jpeg10 = ...
    CAR_DFWB_s126w7_SwinIR_M_jpeg20 = ...
    CAR_DFWB_s126w7_SwinIR_M_jpeg30 = ...
    CAR_DFWB_s126w7_SwinIR_M_jpeg40 = ...
    colorCAR_DFWB_s126w7_SwinIR_M_jpeg10 = ...
    colorCAR_DFWB_s126w7_SwinIR_M_jpeg20 = ...
    colorCAR_DFWB_s126w7_SwinIR_M_jpeg30 = ...
    colorCAR_DFWB_s126w7_SwinIR_M_jpeg40 = ...

def SwinIR(
    clip: vs.VideoNode,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: SwinIRModel = ...,
    backend: backendT = ...,
) -> vs.VideoNode:
    """SwinIR: Image Restoration Using Swin Transformer"""
    ...

@enum.unique
class ArtCNNModel(enum.IntEnum):
    ArtCNN_C4F32 = ...
    ArtCNN_C4F32_DS = ...
    ArtCNN_C16F64 = ...
    ArtCNN_C16F64_DS = ...
    ArtCNN_C4F32_Chroma = ...
    ArtCNN_C16F64_Chroma = ...
    ArtCNN_R16F96 = ...
    ArtCNN_R8F64 = ...
    ArtCNN_R8F64_DS = ...
    ArtCNN_R8F64_Chroma = ...
    ArtCNN_C4F16 = ...
    ArtCNN_C4F16_DS = ...

def ArtCNN(
    clip: vs.VideoNode,
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    overlap: typing.Optional[typing.Union[int, typing.Tuple[int, int]]] = ...,
    model: ArtCNNModel = ...,
    backend: backendT = ...,
) -> vs.VideoNode:
    """ArtCNN (https://github.com/Artoriuz/ArtCNN)"""
    ...

def get_engine_path(
    network_path: str,
    min_shapes: typing.Tuple[int, int],
    opt_shapes: typing.Tuple[int, int],
    max_shapes: typing.Tuple[int, int],
    workspace: typing.Optional[int],
    fp16: bool,
    device_id: int,
    use_cublas: bool,
    static_shape: bool,
    tf32: bool,
    use_cudnn: bool,
    input_format: int,
    output_format: int,
    builder_optimization_level: int,
    max_aux_streams: typing.Optional[int],
    short_path: typing.Optional[bool],
    bf16: bool,
    engine_folder: typing.Optional[str],
) -> str: ...
def trtexec(
    network_path: str,
    channels: int,
    opt_shapes: typing.Tuple[int, int],
    max_shapes: typing.Tuple[int, int],
    fp16: bool,
    device_id: int,
    workspace: typing.Optional[int] = ...,
    verbose: bool = ...,
    use_cuda_graph: bool = ...,
    use_cublas: bool = ...,
    static_shape: bool = ...,
    tf32: bool = ...,
    log: bool = ...,
    use_cudnn: bool = ...,
    use_edge_mask_convolutions: bool = ...,
    use_jit_convolutions: bool = ...,
    heuristic: bool = ...,
    input_name: str = ...,
    input_format: int = ...,
    output_format: int = ...,
    min_shapes: typing.Tuple[int, int] = ...,
    faster_dynamic_shapes: bool = ...,
    force_fp16: bool = ...,
    builder_optimization_level: int = ...,
    max_aux_streams: typing.Optional[int] = ...,
    short_path: typing.Optional[bool] = ...,
    bf16: bool = ...,
    custom_env: typing.Dict[str, str] = ...,
    custom_args: typing.List[str] = ...,
    engine_folder: typing.Optional[str] = ...,
    max_tactics: typing.Optional[int] = ...,
    tiling_optimization_level: int = ...,
    l2_limit_for_tiling: int = ...,
) -> str: ...
def get_mxr_path(
    network_path: str,
    opt_shapes: typing.Tuple[int, int],
    fp16: bool,
    fast_math: bool,
    exhaustive_tune: bool,
    device_id: int,
    short_path: typing.Optional[bool],
) -> str: ...
def migraphx_driver(
    network_path: str,
    channels: int,
    opt_shapes: typing.Tuple[int, int],
    fp16: bool,
    fast_math: bool,
    exhaustive_tune: bool,
    device_id: int,
    input_name: str = ...,
    short_path: typing.Optional[bool] = ...,
    custom_env: typing.Dict[str, str] = ...,
    custom_args: typing.List[str] = ...,
) -> str: ...
def tensorrt_rtx(
    network_path: str,
    channels: int,
    shapes: typing.Tuple[int, int],
    fp16: bool,
    device_id: int,
    workspace: typing.Optional[int] = ...,
    verbose: bool = ...,
    use_cuda_graph: bool = ...,
    use_edge_mask_convolutions: bool = ...,
    input_name: str = ...,
    builder_optimization_level: int = ...,
    max_aux_streams: typing.Optional[int] = ...,
    short_path: typing.Optional[bool] = ...,
    custom_env: typing.Dict[str, str] = ...,
    custom_args: typing.List[str] = ...,
    engine_folder: typing.Optional[str] = ...,
    max_tactics: typing.Optional[int] = ...,
    tiling_optimization_level: int = ...,
    l2_limit_for_tiling: int = ...,
) -> str: ...
def calc_size(width: int, tiles: int, overlap: int, multiple: int = ...) -> int: ...
def calc_tilesize(
    tiles: typing.Optional[typing.Union[int, typing.Tuple[int, int]]],
    tilesize: typing.Optional[typing.Union[int, typing.Tuple[int, int]]],
    width: int,
    height: int,
    multiple: int,
    overlap_w: int,
    overlap_h: int,
) -> typing.Tuple[typing.Tuple[int, int], typing.Tuple[int, int]]: ...
def init_backend(
    backend: backendT, trt_opt_shapes: typing.Tuple[int, int]
) -> backendT: ...
def inference_with_fallback(
    clips: typing.List[vs.VideoNode],
    network_path: typing.Union[bytes, str],
    overlap: typing.Tuple[int, int],
    tilesize: typing.Tuple[int, int],
    backend: backendT,
    path_is_serialization: bool = ...,
    input_name: str = ...,
    batch_size: int = ...,
) -> vs.VideoNode: ...
def inference(
    clips: typing.Union[vs.VideoNode, typing.List[vs.VideoNode]],
    network_path: str,
    overlap: typing.Tuple[int, int] = ...,
    tilesize: typing.Optional[typing.Tuple[int, int]] = ...,
    backend: backendT = ...,
    input_name: typing.Optional[str] = ...,
    batch_size: int = ...,
) -> vs.VideoNode: ...
def flexible_inference_with_fallback(
    clips: typing.List[vs.VideoNode],
    network_path: typing.Union[bytes, str],
    overlap: typing.Tuple[int, int],
    tilesize: typing.Tuple[int, int],
    backend: backendT,
    path_is_serialization: bool = ...,
    input_name: str = ...,
    flexible_output_prop: str = ...,
    batch_size: int = ...,
) -> typing.List[vs.VideoNode]: ...
def flexible_inference(
    clips: typing.Union[vs.VideoNode, typing.List[vs.VideoNode]],
    network_path: str,
    overlap: typing.Tuple[int, int] = ...,
    tilesize: typing.Optional[typing.Tuple[int, int]] = ...,
    backend: backendT = ...,
    input_name: typing.Optional[str] = ...,
    flexible_output_prop: str = ...,
    batch_size: int = ...,
) -> typing.List[vs.VideoNode]: ...
def get_input_name(network_path: str) -> str: ...
def bits_as(clip: vs.VideoNode, target: vs.VideoNode) -> vs.VideoNode: ...

class BackendV2:
    """simplified backend interfaces with keyword-only arguments

    More exposed arguments may be added for each backend,
    but existing ones will always function in a forward compatible way.
    """

    @staticmethod
    def TRT(
        *,
        num_streams: int = ...,
        fp16: bool = ...,
        tf32: bool = ...,
        output_format: int = ...,
        workspace: typing.Optional[int] = ...,
        use_cuda_graph: bool = ...,
        static_shape: bool = ...,
        min_shapes: typing.Tuple[int, int] = ...,
        opt_shapes: typing.Optional[typing.Tuple[int, int]] = ...,
        max_shapes: typing.Optional[typing.Tuple[int, int]] = ...,
        force_fp16: bool = ...,
        use_cublas: bool = ...,
        use_cudnn: bool = ...,
        device_id: int = ...,
        **kwargs
    ) -> Backend.TRT: ...
    @staticmethod
    def NCNN_VK(
        *, num_streams: int = ..., fp16: bool = ..., device_id: int = ..., **kwargs
    ) -> Backend.NCNN_VK: ...
    @staticmethod
    def ORT_CUDA(
        *,
        num_streams: int = ...,
        fp16: bool = ...,
        cudnn_benchmark: bool = ...,
        device_id: int = ...,
        **kwargs
    ) -> Backend.ORT_CUDA: ...
    @staticmethod
    def OV_CPU(
        *,
        num_streams: typing.Union[int, str] = ...,
        bf16: bool = ...,
        bind_thread: bool = ...,
        num_threads: int = ...,
        **kwargs
    ) -> Backend.OV_CPU: ...
    @staticmethod
    def ORT_CPU(*, num_streams: int = ..., **kwargs) -> Backend.ORT_CPU: ...
    @staticmethod
    def OV_GPU(
        *,
        num_streams: typing.Union[int, str] = ...,
        fp16: bool = ...,
        device_id: int = ...,
        **kwargs
    ) -> Backend.OV_GPU: ...
    @staticmethod
    def ORT_DML(
        *, device_id: int = ..., num_streams: int = ..., fp16: bool = ..., **kwargs
    ) -> Backend.ORT_DML: ...
    @staticmethod
    def MIGX(
        *,
        fp16: bool = ...,
        opt_shapes: typing.Optional[typing.Tuple[int, int]] = ...,
        **kwargs
    ) -> Backend.MIGX: ...
    @staticmethod
    def OV_NPU(**kwargs) -> Backend.OV_NPU: ...
    @staticmethod
    def ORT_COREML(
        *, num_streams: int = ..., fp16: bool = ..., **kwargs
    ) -> Backend.ORT_COREML: ...
    @staticmethod
    def TRT_RTX(
        *,
        num_streams: int = ...,
        fp16: bool = ...,
        workspace: typing.Optional[int] = ...,
        use_cuda_graph: bool = ...,
        device_id: int = ...,
        **kwargs
    ) -> Backend.TRT_RTX: ...

def fmtc_resample(clip: vs.VideoNode, **kwargs) -> vs.VideoNode: ...
def parse_trt_version(version: int) -> typing.Tuple[int, int, int]: ...
